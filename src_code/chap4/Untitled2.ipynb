{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from numpy import *\n",
    "from bayes import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problem', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthing', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stupid',\n",
       " 'ate',\n",
       " 'has',\n",
       " 'help',\n",
       " 'him',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'stop',\n",
       " 'food',\n",
       " 'quit',\n",
       " 'love',\n",
       " 'flea',\n",
       " 'mr',\n",
       " 'licks',\n",
       " 'buying',\n",
       " 'not',\n",
       " 'park',\n",
       " 'worthless',\n",
       " 'is',\n",
       " 'please',\n",
       " 'steak',\n",
       " 'problem',\n",
       " 'how',\n",
       " 'take',\n",
       " 'garbage',\n",
       " 'posting',\n",
       " 'cute',\n",
       " 'maybe',\n",
       " 'to',\n",
       " 'worthing',\n",
       " 'my',\n",
       " 'I',\n",
       " 'dog']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = array(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 33)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]]), [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(trainMat), listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)        # P(1)的概率，为二分类问题，则有P(0) = 1 - P(1)\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] ==1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25809654, -2.56494936, -2.56494936, -2.56494936, -2.15948425,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -3.25809654, -3.25809654, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -3.25809654, -2.56494936, -3.25809654, -2.56494936, -3.25809654,\n",
       "       -1.87180218, -2.56494936, -2.56494936])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.65822808, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -2.35137526, -2.35137526, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "       -2.35137526, -3.04452244, -2.35137526, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -1.94591015])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "8.0\n",
      "15.0\n",
      "13.0\n",
      "24.0\n",
      "19.0\n",
      "19.0 24.0\n"
     ]
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + (1 - log(pClass1))\n",
    "    if  p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texParse('D:\\Projects_Python\\DataSets\\machinelearninginaction\\Ch04\\email\\ham\\6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects_Python\\DataSets\\machinelearninginaction\\Ch04\\email\\ham\u0006.txt\n"
     ]
    }
   ],
   "source": [
    "print('D:\\Projects_Python\\DataSets\\machinelearninginaction\\Ch04\\email\\ham\\6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[...], [...], [...], [...], [...], [...], [...]]\n"
     ]
    }
   ],
   "source": [
    "fr = open('6.txt','r')\n",
    "line = []\n",
    "for li in fr:\n",
    "    line.append(line)\n",
    "fr.close\n",
    "print(line)\n",
    "# print(fr.readlines)\n",
    "# print( line for line in fr.readlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[...], [...], [...], [...], [...], [...], [...]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='6.txt' mode='r' encoding='cp936'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = open('6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.readlines>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.readlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,\\n',\n",
       " '\\n',\n",
       " 'Since you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\\n',\n",
       " '\\n',\n",
       " 'For example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you抮e just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\\n',\n",
       " '\\n',\n",
       " 'you have received this mandatory email service announcement to update you about important changes to Google Groups.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "\n",
      "Since you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\n",
      "\n",
      "For example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you抮e just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\n",
      "\n",
      "you have received this mandatory email service announcement to update you about important changes to Google Groups. <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "fr = open('6.txt')\n",
    "list1 = []\n",
    "l4 = fr.read()\n",
    "print(l4, type(l4))\n",
    "for line in fr.readlines():\n",
    "    line = line.split()\n",
    "#     line = regEx.split(line)\n",
    "    list1 += (line)\n",
    "L3 = fr.read()\n",
    "l2 = texParse(l4)\n",
    "fr.close()\n",
    "# [tok.lower() for tok in list1 if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "emailText = open('6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,\\n\\nSince you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\\n\\nFor example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you抮e just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\\n\\nyou have received this mandatory email service announcement to update you about important changes to Google Groups.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Since',\n",
       " 'you',\n",
       " 'are',\n",
       " 'an',\n",
       " 'owner',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " 'one',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " 'group',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'customized',\n",
       " 'welcome',\n",
       " 'message',\n",
       " 'pages',\n",
       " 'or',\n",
       " 'files',\n",
       " 'we',\n",
       " 'are',\n",
       " 'writing',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'you',\n",
       " 'that',\n",
       " 'we',\n",
       " 'will',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'be',\n",
       " 'supporting',\n",
       " 'these',\n",
       " 'features',\n",
       " 'starting',\n",
       " 'February',\n",
       " '2011',\n",
       " 'We',\n",
       " 'made',\n",
       " 'this',\n",
       " 'decision',\n",
       " 'so',\n",
       " 'that',\n",
       " 'we',\n",
       " 'can',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'core',\n",
       " 'functionalities',\n",
       " 'of',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " 'mailing',\n",
       " 'lists',\n",
       " 'and',\n",
       " 'forum',\n",
       " 'discussions',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'these',\n",
       " 'features',\n",
       " 'we',\n",
       " 'encourage',\n",
       " 'you',\n",
       " 'to',\n",
       " 'use',\n",
       " 'products',\n",
       " 'that',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'specifically',\n",
       " 'for',\n",
       " 'file',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'page',\n",
       " 'creation',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Google',\n",
       " 'Docs',\n",
       " 'and',\n",
       " 'Google',\n",
       " 'Sites',\n",
       " 'For',\n",
       " 'example',\n",
       " 'you',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'create',\n",
       " 'your',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'Google',\n",
       " 'Sites',\n",
       " 'and',\n",
       " 'share',\n",
       " 'the',\n",
       " 'site',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '174623',\n",
       " 'with',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'your',\n",
       " 'group',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'store',\n",
       " 'your',\n",
       " 'files',\n",
       " 'on',\n",
       " 'the',\n",
       " 'site',\n",
       " 'by',\n",
       " 'attaching',\n",
       " 'files',\n",
       " 'to',\n",
       " 'pages',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '90563',\n",
       " 'on',\n",
       " 'the',\n",
       " 'site',\n",
       " 'If',\n",
       " 'you抮e',\n",
       " 'just',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'place',\n",
       " 'to',\n",
       " 'upload',\n",
       " 'your',\n",
       " 'files',\n",
       " 'so',\n",
       " 'that',\n",
       " 'your',\n",
       " 'group',\n",
       " 'members',\n",
       " 'can',\n",
       " 'download',\n",
       " 'them',\n",
       " 'we',\n",
       " 'suggest',\n",
       " 'you',\n",
       " 'try',\n",
       " 'Google',\n",
       " 'Docs',\n",
       " 'You',\n",
       " 'can',\n",
       " 'upload',\n",
       " 'files',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '50092',\n",
       " 'and',\n",
       " 'share',\n",
       " 'access',\n",
       " 'with',\n",
       " 'either',\n",
       " 'a',\n",
       " 'group',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '66343',\n",
       " 'or',\n",
       " 'an',\n",
       " 'individual',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'py',\n",
       " 'hl',\n",
       " 'en',\n",
       " 'answer',\n",
       " '86152',\n",
       " 'assigning',\n",
       " 'either',\n",
       " 'edit',\n",
       " 'or',\n",
       " 'download',\n",
       " 'only',\n",
       " 'access',\n",
       " 'to',\n",
       " 'the',\n",
       " 'files',\n",
       " 'you',\n",
       " 'have',\n",
       " 'received',\n",
       " 'this',\n",
       " 'mandatory',\n",
       " 'email',\n",
       " 'service',\n",
       " 'announcement',\n",
       " 'to',\n",
       " 'update',\n",
       " 'you',\n",
       " 'about',\n",
       " 'important',\n",
       " 'changes',\n",
       " 'to',\n",
       " 'Google',\n",
       " 'Groups',\n",
       " '']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(random.uniform(0, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = range(50)[29]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 50)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 50)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'D:\\Projects_Python\\DataSets\\machinelearninginaction\\Ch04\\email\\spam'\n",
    "path2 = 'D:\\Projects_Python\\DataSets\\machinelearninginaction\\Ch04\\email\\ham'\n",
    "\n",
    "def spamTest():\n",
    "    numdir = len(os.listdir(path1))\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, numdir):\n",
    "        wordList = texParse(open(path1 + '\\%d.txt' % i).read())    # 某一文档的字符列表\n",
    "        docList.append(wordList)                  # 将所得字符列表按列表形式存放到docListzhong\n",
    "        fullText.extend(wordList)                 # 将所得字符列表按字符形式存放到fullText中\n",
    "        classList.append(1)\n",
    "        wordList = texParse(open(path2 + '\\%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)           # docList为整个文档（全部邮件），文档每行为一份邮件中的全部字符,\n",
    "                                                   # 同时将整个文档中出现的字符整理为一个字符向量\n",
    "    trainingSet = range(50)\n",
    "    # ********随机选择测试集*******\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    ## *******随机选取训练集*******\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    poV, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is :', float(errorCount) / len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xae in position 199: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-dea3e8b50c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspamTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-221-4da7aaccaaed>\u001b[0m in \u001b[0;36mspamTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfullText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# 将所得字符列表按字符形式存放到fullText中\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mclassList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\%d.txt'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdocList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mfullText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xae in position 199: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
